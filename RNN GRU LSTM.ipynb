{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‰ΩôÁõàËììÔºõË™ûÁ¢©‰∫åÔºõ110555009**\\\n",
    "**111-2 Computational Linguistics**\n",
    "<p align=\"center\", style = \"font-size:18pt\">\n",
    "<b>7th Assignment<br>\n",
    "Deep Learning</b>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 1. Ë≥áÊñôËÆÄÂÖ•</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "\n",
    "# preprocess the dataset: remove all the <br /> tag\n",
    "\n",
    "def prepareData(dir):\n",
    "  data = text_dataset_from_directory(dir) # load in dataset\n",
    "  return data.map(\n",
    "    lambda text, label: (regex_replace(text, '<br />', ' '), label),\n",
    "  )\n",
    "\n",
    "\n",
    "# load in dataset\n",
    "train_data = prepareData(\"movie-reviews-dataset/train\")\n",
    "test_data = prepareData(\"movie-reviews-dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This is the movie that I use to judge all other bad movies, and so far there hasn't been anything close.  The only good thing I can say is that after watching this I know that I have seen the worst movie I will ever see.\"\n",
      "0\n",
      "b'The real star of this ridiculous story is glorious technicolor. A visual treat to the eye, the film fails to stimulate the mind and heart. I was intrigued, at first, by the idea of Dietrich and Boyer leaving religion in order to \"find\" their capacity for love. What follows is a huge disappointment. Boyer is the only real actor in the production and one feels his torment. Dietrich\\'s amazing wardrobe outshines her performance -- at times her face is frightening to look at -- a unfeeling mask. As a monk, Boyer held the formula for the monastery\\'s liquer (which reminds me of the true story of Chartreuse) -- when he leaves his \"marriage to god\" the reaction by his fellow monks holds the shock and fear that perpetuate organized religion. The viewer feels Boyer was well rid of his past. However, the journey that follows is all too predictable.'\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_data.take(2):\n",
    "  print(text_batch.numpy()[0])\n",
    "  print(label_batch.numpy()[0]) # 0 = negative, 1 = positive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 2. ÂêëÈáèÂåñÊñáÂ≠ó</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "max_tokens = 1000\n",
    "max_len = 50\n",
    "vectorize_layer = TextVectorization(\n",
    "  max_tokens=max_tokens,\n",
    "  output_mode=\"int\",\n",
    "  output_sequence_length=max_len,\n",
    ")\n",
    "\n",
    "train_texts = train_data.map(lambda text, label: text) \n",
    "vectorize_layer.adapt(train_texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 3. Ê®°ÂûãË®ìÁ∑¥</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "‚ú® <b>GRU</b><br>\n",
    "&emsp;&ensp;‚Üí ÂàùÂßãÂåñÊ®°Âûã\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model = Sequential(name=\"GRU-model\")\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))\n",
    "model.add(vectorize_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;‚Üí Âä†ÂÖ•embedding\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model.add(Embedding(max_tokens + 1, 128))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;‚Üí Âä†ÂÖ•GRU layer\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model.add(GRU(64))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;‚Üí Âä†ÂÖ•‰∏ÄÂÄãhidden layerË∑üÊúÄÂæåÁöÑoutput layer\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;‚Üí ÂÆåÊàêÊ®°Âûã\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GRU-model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 50)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 50, 128)           128128    \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169,601\n",
      "Trainable params: 169,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;‚Üí ÊîæÂÖ•Ë®ìÁ∑¥ÈõÜË®ìÁ∑¥Ê®°Âûã\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 14s 16ms/step - loss: 0.5833 - accuracy: 0.6781\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.5028 - accuracy: 0.7532\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.4714 - accuracy: 0.7702\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 0.4499 - accuracy: 0.7810\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.4336 - accuracy: 0.7891\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.4170 - accuracy: 0.7977\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.3996 - accuracy: 0.8035\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.3801 - accuracy: 0.8108\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.3633 - accuracy: 0.8193\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.3416 - accuracy: 0.8304\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.3229 - accuracy: 0.8405\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.3039 - accuracy: 0.8494\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 25s 31ms/step - loss: 0.2810 - accuracy: 0.8605\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.2600 - accuracy: 0.8708\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.2503 - accuracy: 0.8764\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.2304 - accuracy: 0.8875\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.2103 - accuracy: 0.8975\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.2003 - accuracy: 0.9051\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.1818 - accuracy: 0.9152\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1618 - accuracy: 0.9253\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1572 - accuracy: 0.9300\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1468 - accuracy: 0.9358\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1279 - accuracy: 0.9455\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.1129 - accuracy: 0.9515\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1088 - accuracy: 0.9557\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.1003 - accuracy: 0.9605\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0845 - accuracy: 0.9669\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.0797 - accuracy: 0.9682\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0645 - accuracy: 0.9761\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0689 - accuracy: 0.9737\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0705 - accuracy: 0.9750\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0503 - accuracy: 0.9819\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.0494 - accuracy: 0.9834\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 32s 40ms/step - loss: 0.0457 - accuracy: 0.9843\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 26s 32ms/step - loss: 0.0469 - accuracy: 0.9848\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.0461 - accuracy: 0.9849\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.0455 - accuracy: 0.9852\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.0362 - accuracy: 0.9876\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0397 - accuracy: 0.9869\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0277 - accuracy: 0.9911\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 0.0425 - accuracy: 0.9862\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0308 - accuracy: 0.9900\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0197 - accuracy: 0.9939\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0324 - accuracy: 0.9899\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.0297 - accuracy: 0.9904\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.0312 - accuracy: 0.9903\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.0344 - accuracy: 0.9885\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.0271 - accuracy: 0.9914\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0188 - accuracy: 0.9943\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0297 - accuracy: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fad5a0c820>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "‚ú® <b>bi-LSTM</b><br>\n",
    "&emsp;&ensp;‚Üí Ë®≠ÂÆöÊ®°ÂûãÂèÉÊï∏\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 50)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 50, 128)           128128    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 235,265\n",
      "Trainable params: 235,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "\n",
    "bilstm_model = Sequential([\n",
    "    Input(shape=(1,), dtype=\"string\"),\n",
    "    vectorize_layer,\n",
    "    Embedding(max_tokens + 1, 128),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "bilstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(bilstm_model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;‚Üí ÊîæÂÖ•Ë®ìÁ∑¥ÈõÜË®ìÁ∑¥Ê®°Âûã\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 19s 22ms/step - loss: 0.5535 - accuracy: 0.7054\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.4865 - accuracy: 0.7562\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.4550 - accuracy: 0.7766\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.4245 - accuracy: 0.7968\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.3916 - accuracy: 0.8151\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.3598 - accuracy: 0.8320\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.3292 - accuracy: 0.8471\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.2988 - accuracy: 0.8619\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.2735 - accuracy: 0.8744\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.2471 - accuracy: 0.8878\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.2239 - accuracy: 0.8977\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.2028 - accuracy: 0.9086\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.1822 - accuracy: 0.9207\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.1666 - accuracy: 0.9269\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.1448 - accuracy: 0.9373\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.1262 - accuracy: 0.9467\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 649s 831ms/step - loss: 0.1188 - accuracy: 0.9521\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 22s 27ms/step - loss: 0.0989 - accuracy: 0.9609\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.0910 - accuracy: 0.9652\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.0770 - accuracy: 0.9720\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 0.0717 - accuracy: 0.9741\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0692 - accuracy: 0.9744\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0566 - accuracy: 0.9792\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.0459 - accuracy: 0.9855\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.0438 - accuracy: 0.9852\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.0458 - accuracy: 0.9844\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.0364 - accuracy: 0.9874\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0367 - accuracy: 0.9875\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0344 - accuracy: 0.9872\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0400 - accuracy: 0.9872\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.0280 - accuracy: 0.9910\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0275 - accuracy: 0.9917\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.0315 - accuracy: 0.9893\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 61s 78ms/step - loss: 0.0241 - accuracy: 0.9920\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 60s 76ms/step - loss: 0.0237 - accuracy: 0.9928\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0225 - accuracy: 0.9924\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 58s 74ms/step - loss: 0.0245 - accuracy: 0.9922\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0278 - accuracy: 0.9913\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.0271 - accuracy: 0.9910\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.0180 - accuracy: 0.9942\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.0212 - accuracy: 0.9934\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 59s 74ms/step - loss: 0.0229 - accuracy: 0.9927\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 59s 76ms/step - loss: 0.0188 - accuracy: 0.9943\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0200 - accuracy: 0.9933\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.0136 - accuracy: 0.9954\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.0210 - accuracy: 0.9932\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0248 - accuracy: 0.9920\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.0174 - accuracy: 0.9941\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.0095 - accuracy: 0.9971\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0158 - accuracy: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1faeac7a640>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_model.fit(train_data, epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "‚ú® <b>bi-RNN</b><br>\n",
    "&emsp;&ensp;‚Üí Ë®≠ÂÆöÊ®°ÂûãÂèÉÊï∏\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 50)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 50, 128)           128128    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 128)              24704     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,153\n",
      "Trainable params: 161,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "birnn_model = Sequential([\n",
    "    Input(shape=(1,), dtype=\"string\"),\n",
    "    vectorize_layer,\n",
    "    Embedding(max_tokens + 1, 128),\n",
    "    Bidirectional(SimpleRNN(64)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "birnn_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(birnn_model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;‚Üí ÊîæÂÖ•Ë®ìÁ∑¥ÈõÜË®ìÁ∑¥Ê®°Âûã\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 12s 14ms/step - loss: 0.6478 - accuracy: 0.6045\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.5452 - accuracy: 0.7261\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.5029 - accuracy: 0.7562\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.4735 - accuracy: 0.7752\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.4223 - accuracy: 0.8080\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.3677 - accuracy: 0.8386\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.3173 - accuracy: 0.8651\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.2774 - accuracy: 0.8820\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.2384 - accuracy: 0.9024\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 24s 31ms/step - loss: 0.2140 - accuracy: 0.9124\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 22s 27ms/step - loss: 0.1790 - accuracy: 0.9275\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.1537 - accuracy: 0.9391\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.1433 - accuracy: 0.9452\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.1327 - accuracy: 0.9493\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 22s 29ms/step - loss: 0.1150 - accuracy: 0.9567\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.1050 - accuracy: 0.9602\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.1069 - accuracy: 0.9594\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.1041 - accuracy: 0.9604\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.0924 - accuracy: 0.9654\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0789 - accuracy: 0.9698\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.0818 - accuracy: 0.9700\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0776 - accuracy: 0.9708\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0865 - accuracy: 0.9682\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.0803 - accuracy: 0.9701\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.0749 - accuracy: 0.9728\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.0786 - accuracy: 0.9714\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.0642 - accuracy: 0.9778\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 29s 36ms/step - loss: 0.0678 - accuracy: 0.9760\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.0654 - accuracy: 0.9769\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.0699 - accuracy: 0.9756\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0574 - accuracy: 0.9797\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0769 - accuracy: 0.9716\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.0528 - accuracy: 0.9813\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.0587 - accuracy: 0.9792\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 29s 36ms/step - loss: 0.0658 - accuracy: 0.9766\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.0556 - accuracy: 0.9808\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.1188 - accuracy: 0.9573\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.0540 - accuracy: 0.9812\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.0487 - accuracy: 0.9830\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.0621 - accuracy: 0.9784\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.0551 - accuracy: 0.9792\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.0542 - accuracy: 0.9805\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.0570 - accuracy: 0.9798\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.0588 - accuracy: 0.9796\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.0535 - accuracy: 0.9812\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.0470 - accuracy: 0.9836\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0541 - accuracy: 0.9807\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 29s 36ms/step - loss: 0.0569 - accuracy: 0.9794\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.0463 - accuracy: 0.9841\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.0539 - accuracy: 0.9812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1faeae73a30>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birnn_model.fit(train_data, epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 4. ‰øùÂ≠òÊ®°ÂûãÂèÉÊï∏</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('GRU')\n",
    "bilstm_model.save_weights('bi-LSTM')\n",
    "birnn_model.save_weights('bi-RNN')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 5. Ê®°ÂûãË©ï‰º∞</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "‚ú® <b>GRU</b><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 7s 9ms/step - loss: 2.8272 - accuracy: 0.6918\n",
      "test loss, test acc: [2.827223539352417, 0.6917999982833862]\n"
     ]
    }
   ],
   "source": [
    "GRU_results = model.evaluate(test_data)\n",
    "print(\"test loss, test acc:\", GRU_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "‚ú® <b>bi-LSTM</b><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 13ms/step - loss: 2.9130 - accuracy: 0.6950\n",
      "test loss, test acc: [2.9129574298858643, 0.6949599981307983]\n"
     ]
    }
   ],
   "source": [
    "LSTM_results = bilstm_model.evaluate(test_data)\n",
    "print(\"test loss, test acc:\", LSTM_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "‚ú® <b>bi-RNN</b><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 8ms/step - loss: 2.0232 - accuracy: 0.6617\n",
      "test loss, test acc: [2.0231945514678955, 0.6616799831390381]\n"
     ]
    }
   ],
   "source": [
    "RNN_results = birnn_model.evaluate(test_data)\n",
    "print(\"test loss, test acc:\", RNN_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\", style = \"font-size:16pt\">\n",
    "    üßê<b>Ë®éË´ñ</b>üßê<br>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "‚ú®ÂàÜÊï∏Áµ±Êï¥<br>\n",
    "</p>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan = \"2\">&emsp;&emsp;</th>\n",
    "    <th colspan = \"2\", style = \"background-color: #F1E1FF\" >training</th>\n",
    "    <th colspan = \"2\", style = \"background-color: #F1E1FF\" >testing</th>\n",
    "  </tr>\n",
    "  <tr style = \"background-color: #FFECF5\">\n",
    "    <th>loss</th>\n",
    "    <th>accuracy</th>\n",
    "    <th>loss</th>\n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style = \"background-color: \t#D2E9FF\">GRU</th>\n",
    "    <td>0.0297</td>\n",
    "    <td>0.9907</td>\n",
    "    <td>2.8272</td>\n",
    "    <td>0.6918</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style = \"background-color: \t#D2E9FF\">bi-RNN</th>\n",
    "    <td>0.0539</td>\n",
    "    <td>0.9812</td>\n",
    "    <td>2.0232</td>\n",
    "    <td>0.6617</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style = \"background-color: \t#D2E9FF\">bi-LSTM</th>\n",
    "    <td >0.0158</td>\n",
    "    <td>0.9945</td>\n",
    "    <td>2.9130</td>\n",
    "    <td>0.6950</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "   &emsp;&emsp;È¶ñÂÖàÂæûÊ®°ÂûãË®ìÁ∑¥ÁöÑÈÅéÁ®ãÂ∞±ÂèØ‰ª•ÁúãÂà∞ÔºåÂú®ÂèÉÊï∏Ë®≠ÂÆöÈÉΩ‰∏ÄÊ®£ÁöÑÊÉÖÊ≥Å‰∏ãÔºåbi-LSTMÁöÑË°®ÁèæÊòØÊúÄÂ•ΩÁöÑ„ÄÇËÄåÊ∏¨Ë©¶ÁöÑÁµêÊûú‰πüÊòØbi-LSTMÁöÑsccuracyÊúÄÈ´òÔºåÂîØ‰∏Ä‰∏çÂêåÊòØbi-LSTMÁöÑlossÊòØÊúÄÈ´òÁöÑ„ÄÇbi-LSTMÁöÑË°®ÁèæÊúÄÂ•ΩÂÖ∂ÂØ¶ÊòØÂèØ‰ª•È†êË¶ãÁöÑÁµêÊûúÔºåÂõ†ÁÇ∫ÂÆÉÊîπÂñÑ‰∫ÜRNNÂèØËÉΩÊääÈï∑Âè•Â≠ê‰∏≠ÊØîËºÉÂâçÈù¢ÁöÑcontextÁ®ÄÈáãÊéâÁöÑÂïèÈ°åÔºåÂèàÊØîGRUÂ§ö‰∫Ü‰∏ÄÂÄãÊñπÂêëÂéªËÄÉÊÖÆÊõ¥Â§öcontextÔºåÊâÄ‰ª•ÁµêÊûúÊâçÊòØÊúÄÂ•ΩÁöÑ„ÄÇÊØîËºÉËÆì‰∫∫ÊÑèÂ§ñÁöÑÊòØ‰ªñÂÄë‰πãÈñìÁöÑÂ∑ÆË∑ùÂÖ∂ÂØ¶Ê≤íÊúâÂæàÂ§ßÔºåÈÄôÂÄãÈÉ®ÂàÜ‰∏çÁ¢∫ÂÆöÊòØÈÄôÂπæÂÄãÊ®°ÂûãÂÖ∂ÂØ¶ÈÉΩÂè™ÊòØÂú®Á¥∞ÁØÄ‰∏äÈáùÂ∞çÁº∫ÈªûÂÅö‰∫Ü‰∏ÄÈªû‰øÆÊîπÁöÑÈóú‰øÇÔºåÈÇÑÊòØË≥áÊñôÁöÑÂïèÈ°å„ÄÇ\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eee8ebf69407a7ef35d4adf15818993a36e6c576e8bb2535d7c9bac5c8386c87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
