{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**余盈蓓；語碩二；110555009**\\\n",
    "**111-2 Computational Linguistics**\n",
    "<p align=\"center\", style = \"font-size:18pt\">\n",
    "<b>7th Assignment<br>\n",
    "Deep Learning</b>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 1. 資料讀入</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "\n",
    "# preprocess the dataset: remove all the <br /> tag\n",
    "\n",
    "def prepareData(dir):\n",
    "  data = text_dataset_from_directory(dir) # load in dataset\n",
    "  return data.map(\n",
    "    lambda text, label: (regex_replace(text, '<br />', ' '), label),\n",
    "  )\n",
    "\n",
    "\n",
    "# load in dataset\n",
    "train_data = prepareData(\"movie-reviews-dataset/train\")\n",
    "test_data = prepareData(\"movie-reviews-dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This is the movie that I use to judge all other bad movies, and so far there hasn't been anything close.  The only good thing I can say is that after watching this I know that I have seen the worst movie I will ever see.\"\n",
      "0\n",
      "b'The real star of this ridiculous story is glorious technicolor. A visual treat to the eye, the film fails to stimulate the mind and heart. I was intrigued, at first, by the idea of Dietrich and Boyer leaving religion in order to \"find\" their capacity for love. What follows is a huge disappointment. Boyer is the only real actor in the production and one feels his torment. Dietrich\\'s amazing wardrobe outshines her performance -- at times her face is frightening to look at -- a unfeeling mask. As a monk, Boyer held the formula for the monastery\\'s liquer (which reminds me of the true story of Chartreuse) -- when he leaves his \"marriage to god\" the reaction by his fellow monks holds the shock and fear that perpetuate organized religion. The viewer feels Boyer was well rid of his past. However, the journey that follows is all too predictable.'\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_data.take(2):\n",
    "  print(text_batch.numpy()[0])\n",
    "  print(label_batch.numpy()[0]) # 0 = negative, 1 = positive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 2. 向量化文字</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "max_tokens = 1000\n",
    "max_len = 50\n",
    "vectorize_layer = TextVectorization(\n",
    "  max_tokens=max_tokens,\n",
    "  output_mode=\"int\",\n",
    "  output_sequence_length=max_len,\n",
    ")\n",
    "\n",
    "train_texts = train_data.map(lambda text, label: text) \n",
    "vectorize_layer.adapt(train_texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 3. 模型訓練</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨ <b>GRU</b><br>\n",
    "&emsp;&ensp;→ 初始化模型\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model = Sequential(name=\"GRU-model\")\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))\n",
    "model.add(vectorize_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;→ 加入embedding\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model.add(Embedding(max_tokens + 1, 128))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;→ 加入GRU layer\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "model.add(GRU(64))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;→ 加入一個hidden layer跟最後的output layer\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;→ 完成模型\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GRU-model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 50)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 50, 128)           128128    \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169,601\n",
      "Trainable params: 169,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;→ 放入訓練集訓練模型\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 14s 16ms/step - loss: 0.5833 - accuracy: 0.6781\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.5028 - accuracy: 0.7532\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.4714 - accuracy: 0.7702\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 0.4499 - accuracy: 0.7810\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.4336 - accuracy: 0.7891\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.4170 - accuracy: 0.7977\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.3996 - accuracy: 0.8035\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.3801 - accuracy: 0.8108\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.3633 - accuracy: 0.8193\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.3416 - accuracy: 0.8304\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.3229 - accuracy: 0.8405\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.3039 - accuracy: 0.8494\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 25s 31ms/step - loss: 0.2810 - accuracy: 0.8605\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.2600 - accuracy: 0.8708\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.2503 - accuracy: 0.8764\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.2304 - accuracy: 0.8875\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.2103 - accuracy: 0.8975\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.2003 - accuracy: 0.9051\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.1818 - accuracy: 0.9152\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1618 - accuracy: 0.9253\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1572 - accuracy: 0.9300\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1468 - accuracy: 0.9358\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1279 - accuracy: 0.9455\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.1129 - accuracy: 0.9515\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1088 - accuracy: 0.9557\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.1003 - accuracy: 0.9605\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0845 - accuracy: 0.9669\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.0797 - accuracy: 0.9682\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0645 - accuracy: 0.9761\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0689 - accuracy: 0.9737\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0705 - accuracy: 0.9750\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0503 - accuracy: 0.9819\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.0494 - accuracy: 0.9834\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 32s 40ms/step - loss: 0.0457 - accuracy: 0.9843\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 26s 32ms/step - loss: 0.0469 - accuracy: 0.9848\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.0461 - accuracy: 0.9849\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 38s 49ms/step - loss: 0.0455 - accuracy: 0.9852\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.0362 - accuracy: 0.9876\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0397 - accuracy: 0.9869\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0277 - accuracy: 0.9911\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 0.0425 - accuracy: 0.9862\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0308 - accuracy: 0.9900\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0197 - accuracy: 0.9939\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0324 - accuracy: 0.9899\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.0297 - accuracy: 0.9904\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.0312 - accuracy: 0.9903\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.0344 - accuracy: 0.9885\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 35s 44ms/step - loss: 0.0271 - accuracy: 0.9914\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0188 - accuracy: 0.9943\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0297 - accuracy: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fad5a0c820>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨ <b>bi-LSTM</b><br>\n",
    "&emsp;&ensp;→ 設定模型參數\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 50)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 50, 128)           128128    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 235,265\n",
      "Trainable params: 235,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "\n",
    "bilstm_model = Sequential([\n",
    "    Input(shape=(1,), dtype=\"string\"),\n",
    "    vectorize_layer,\n",
    "    Embedding(max_tokens + 1, 128),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "bilstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(bilstm_model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;→ 放入訓練集訓練模型\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 19s 22ms/step - loss: 0.5535 - accuracy: 0.7054\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.4865 - accuracy: 0.7562\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.4550 - accuracy: 0.7766\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.4245 - accuracy: 0.7968\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.3916 - accuracy: 0.8151\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.3598 - accuracy: 0.8320\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.3292 - accuracy: 0.8471\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.2988 - accuracy: 0.8619\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.2735 - accuracy: 0.8744\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.2471 - accuracy: 0.8878\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.2239 - accuracy: 0.8977\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.2028 - accuracy: 0.9086\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.1822 - accuracy: 0.9207\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.1666 - accuracy: 0.9269\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.1448 - accuracy: 0.9373\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.1262 - accuracy: 0.9467\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 649s 831ms/step - loss: 0.1188 - accuracy: 0.9521\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 22s 27ms/step - loss: 0.0989 - accuracy: 0.9609\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.0910 - accuracy: 0.9652\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.0770 - accuracy: 0.9720\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 0.0717 - accuracy: 0.9741\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0692 - accuracy: 0.9744\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0566 - accuracy: 0.9792\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.0459 - accuracy: 0.9855\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.0438 - accuracy: 0.9852\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.0458 - accuracy: 0.9844\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 56s 72ms/step - loss: 0.0364 - accuracy: 0.9874\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0367 - accuracy: 0.9875\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0344 - accuracy: 0.9872\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0400 - accuracy: 0.9872\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.0280 - accuracy: 0.9910\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0275 - accuracy: 0.9917\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 52s 66ms/step - loss: 0.0315 - accuracy: 0.9893\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 61s 78ms/step - loss: 0.0241 - accuracy: 0.9920\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 60s 76ms/step - loss: 0.0237 - accuracy: 0.9928\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0225 - accuracy: 0.9924\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 58s 74ms/step - loss: 0.0245 - accuracy: 0.9922\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0278 - accuracy: 0.9913\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.0271 - accuracy: 0.9910\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.0180 - accuracy: 0.9942\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.0212 - accuracy: 0.9934\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 59s 74ms/step - loss: 0.0229 - accuracy: 0.9927\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 59s 76ms/step - loss: 0.0188 - accuracy: 0.9943\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0200 - accuracy: 0.9933\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.0136 - accuracy: 0.9954\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.0210 - accuracy: 0.9932\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 56s 71ms/step - loss: 0.0248 - accuracy: 0.9920\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 57s 73ms/step - loss: 0.0174 - accuracy: 0.9941\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.0095 - accuracy: 0.9971\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 57s 72ms/step - loss: 0.0158 - accuracy: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1faeac7a640>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_model.fit(train_data, epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨ <b>bi-RNN</b><br>\n",
    "&emsp;&ensp;→ 設定模型參數\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 50)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 50, 128)           128128    \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 128)              24704     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,153\n",
      "Trainable params: 161,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "birnn_model = Sequential([\n",
    "    Input(shape=(1,), dtype=\"string\"),\n",
    "    vectorize_layer,\n",
    "    Embedding(max_tokens + 1, 128),\n",
    "    Bidirectional(SimpleRNN(64)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "birnn_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(birnn_model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;→ 放入訓練集訓練模型\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 12s 14ms/step - loss: 0.6478 - accuracy: 0.6045\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.5452 - accuracy: 0.7261\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.5029 - accuracy: 0.7562\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.4735 - accuracy: 0.7752\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.4223 - accuracy: 0.8080\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.3677 - accuracy: 0.8386\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.3173 - accuracy: 0.8651\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.2774 - accuracy: 0.8820\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.2384 - accuracy: 0.9024\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 24s 31ms/step - loss: 0.2140 - accuracy: 0.9124\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 22s 27ms/step - loss: 0.1790 - accuracy: 0.9275\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.1537 - accuracy: 0.9391\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.1433 - accuracy: 0.9452\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 0.1327 - accuracy: 0.9493\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 22s 29ms/step - loss: 0.1150 - accuracy: 0.9567\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.1050 - accuracy: 0.9602\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.1069 - accuracy: 0.9594\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.1041 - accuracy: 0.9604\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.0924 - accuracy: 0.9654\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0789 - accuracy: 0.9698\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.0818 - accuracy: 0.9700\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0776 - accuracy: 0.9708\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0865 - accuracy: 0.9682\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.0803 - accuracy: 0.9701\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.0749 - accuracy: 0.9728\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.0786 - accuracy: 0.9714\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 21s 27ms/step - loss: 0.0642 - accuracy: 0.9778\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 29s 36ms/step - loss: 0.0678 - accuracy: 0.9760\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.0654 - accuracy: 0.9769\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.0699 - accuracy: 0.9756\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0574 - accuracy: 0.9797\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0769 - accuracy: 0.9716\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.0528 - accuracy: 0.9813\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.0587 - accuracy: 0.9792\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 29s 36ms/step - loss: 0.0658 - accuracy: 0.9766\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.0556 - accuracy: 0.9808\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.1188 - accuracy: 0.9573\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.0540 - accuracy: 0.9812\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.0487 - accuracy: 0.9830\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.0621 - accuracy: 0.9784\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.0551 - accuracy: 0.9792\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.0542 - accuracy: 0.9805\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.0570 - accuracy: 0.9798\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.0588 - accuracy: 0.9796\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.0535 - accuracy: 0.9812\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.0470 - accuracy: 0.9836\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0541 - accuracy: 0.9807\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 29s 36ms/step - loss: 0.0569 - accuracy: 0.9794\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.0463 - accuracy: 0.9841\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.0539 - accuracy: 0.9812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1faeae73a30>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birnn_model.fit(train_data, epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 4. 保存模型參數</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('GRU')\n",
    "bilstm_model.save_weights('bi-LSTM')\n",
    "birnn_model.save_weights('bi-RNN')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 5. 模型評估</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨ <b>GRU</b><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 7s 9ms/step - loss: 2.8272 - accuracy: 0.6918\n",
      "test loss, test acc: [2.827223539352417, 0.6917999982833862]\n"
     ]
    }
   ],
   "source": [
    "GRU_results = model.evaluate(test_data)\n",
    "print(\"test loss, test acc:\", GRU_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨ <b>bi-LSTM</b><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 13ms/step - loss: 2.9130 - accuracy: 0.6950\n",
      "test loss, test acc: [2.9129574298858643, 0.6949599981307983]\n"
     ]
    }
   ],
   "source": [
    "LSTM_results = bilstm_model.evaluate(test_data)\n",
    "print(\"test loss, test acc:\", LSTM_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨ <b>bi-RNN</b><br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 8ms/step - loss: 2.0232 - accuracy: 0.6617\n",
      "test loss, test acc: [2.0231945514678955, 0.6616799831390381]\n"
     ]
    }
   ],
   "source": [
    "RNN_results = birnn_model.evaluate(test_data)\n",
    "print(\"test loss, test acc:\", RNN_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\", style = \"font-size:16pt\">\n",
    "    🧐<b>討論</b>🧐<br>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨分數統整<br>\n",
    "</p>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan = \"2\">&emsp;&emsp;</th>\n",
    "    <th colspan = \"2\", style = \"background-color: #F1E1FF\" >training</th>\n",
    "    <th colspan = \"2\", style = \"background-color: #F1E1FF\" >testing</th>\n",
    "  </tr>\n",
    "  <tr style = \"background-color: #FFECF5\">\n",
    "    <th>loss</th>\n",
    "    <th>accuracy</th>\n",
    "    <th>loss</th>\n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style = \"background-color: \t#D2E9FF\">GRU</th>\n",
    "    <td>0.0297</td>\n",
    "    <td>0.9907</td>\n",
    "    <td>2.8272</td>\n",
    "    <td>0.6918</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style = \"background-color: \t#D2E9FF\">bi-RNN</th>\n",
    "    <td>0.0539</td>\n",
    "    <td>0.9812</td>\n",
    "    <td>2.0232</td>\n",
    "    <td>0.6617</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th style = \"background-color: \t#D2E9FF\">bi-LSTM</th>\n",
    "    <td >0.0158</td>\n",
    "    <td>0.9945</td>\n",
    "    <td>2.9130</td>\n",
    "    <td>0.6950</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "   &emsp;&emsp;首先從模型訓練的過程就可以看到，在參數設定都一樣的情況下，bi-LSTM的表現是最好的。而測試的結果也是bi-LSTM的sccuracy最高，唯一不同是bi-LSTM的loss是最高的。bi-LSTM的表現最好其實是可以預見的結果，因為它改善了RNN可能把長句子中比較前面的context稀釋掉的問題，又比GRU多了一個方向去考慮更多context，所以結果才是最好的。比較讓人意外的是他們之間的差距其實沒有很大，這個部分不確定是這幾個模型其實都只是在細節上針對缺點做了一點修改的關係，還是資料的問題。\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eee8ebf69407a7ef35d4adf15818993a36e6c576e8bb2535d7c9bac5c8386c87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
