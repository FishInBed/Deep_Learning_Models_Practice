{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**余盈蓓；語碩二；110555009**\\\n",
    "**111-2 Computational Linguistics**\n",
    "<p align=\"center\", style = \"font-size:18pt\">\n",
    "<b>4th Assignment<br>\n",
    "Embeddings and Weightings</b>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:16pt\", align=\"center\">\n",
    "<b>TF-IDF</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\", align=\"center\">\n",
    "下面用魯迅的短篇小說集<a href = \"https://zh.wikisource.org/wiki/%E6%95%85%E4%BA%8B%E6%96%B0%E7%B7%A8\" target=\"_blank\">《故事新編》</a>中的8個章節來計算TF-IDF。\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨資料讀入\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'女媧忽然醒來了。\\n\\n伊似乎是從夢中惊醒的，然而已經記不清做了什麼夢；只是很懊惱，覺得有什麼不足，又覺'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_story = open(\"故事新編.txt\").read()\n",
    "data_story[0:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨章節分割\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'女媧忽然醒來了。\\n\\n伊似乎是從夢中惊醒的，然而已經記不清做了什麼夢；只是很懊惱，覺得有什麼不足，又覺'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_list = data_story.split(\"-end-\")\n",
    "story_list[0][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(story_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨斷詞\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = open(\"Chinese_stopwords.txt\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c5227\\anaconda3\\lib\\site-packages\\ckiptagger\\model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'女媧 醒來 。 \\n\\n伊 似乎 夢 中 惊 醒 已經 記不清 做 夢 ； 懊惱 覺得 不足 覺得 太多'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckiptagger import WS\n",
    "\n",
    "ws = WS(\"D:\\In Bed\\研究所\\碩二下\\計畫\\data\")\n",
    "story_corpus = []\n",
    "\n",
    "for story in story_list:\n",
    "    tokens = ws([story])\n",
    "    clean_tokens=[]\n",
    "    for i in tokens[0]:\n",
    "        if i not in stopword_list:\n",
    "            clean_tokens.append(i)\n",
    "    story_corpus.append(\" \".join(clean_tokens))\n",
    "\n",
    "story_corpus[0][0:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨資料清洗\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(story_corpus)):\n",
    "    story_corpus[i] = re.sub(\"\\n+\", \"\", story_corpus[i])\n",
    "    story_corpus[i] = re.sub(\"[A-z]+ \", \" \", story_corpus[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨生成TF-IDF\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['䀹眼', '一下', '一下子', '一些', '一個個', '一共', '一千', '一千多', '一半', '一口咬定',\n",
       "       '一口面', '一同', '一向', '一味', '一團糟', '一夫', '一字', '一定', '一平', '一年到頭',\n",
       "       '一律', '一手', '一搭又發', '一早', '一時', '一會', '一月', '一望', '一段落', '一溜煙',\n",
       "       '一無所有', '一熔化', '一百多', '一百零三四', '一直', '一瞬息', '一種', '一絲不掛', '一經',\n",
       "       '一聲不響', '一致', '一色', '一萬', '一角', '一起', '一路', '一轉眼間', '一通', '一陣',\n",
       "       '一面'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(story_corpus)\n",
    "story_words = tfidf_vectorizer.get_feature_names_out()\n",
    "story_words[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>䀹眼</th>\n",
       "      <th>一下</th>\n",
       "      <th>一下子</th>\n",
       "      <th>一些</th>\n",
       "      <th>一個個</th>\n",
       "      <th>一共</th>\n",
       "      <th>一千</th>\n",
       "      <th>一千多</th>\n",
       "      <th>一半</th>\n",
       "      <th>一口咬定</th>\n",
       "      <th>...</th>\n",
       "      <th>鼻准骨</th>\n",
       "      <th>鼻子</th>\n",
       "      <th>鼻尖</th>\n",
       "      <th>齊物論</th>\n",
       "      <th>齷齪</th>\n",
       "      <th>龍位</th>\n",
       "      <th>龍准</th>\n",
       "      <th>龍心</th>\n",
       "      <th>龍眉</th>\n",
       "      <th>龜山</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025556</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028626</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015784</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.033631</td>\n",
       "      <td>0.035359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01768</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01768</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016699</td>\n",
       "      <td>0.018671</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.047808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017768</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 4742 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         䀹眼        一下       一下子        一些      一個個        一共        一千  \\\n",
       "0  0.028626  0.000000  0.000000  0.025556  0.00000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.022074  0.00000  0.000000  0.000000   \n",
       "2  0.000000  0.035339  0.000000  0.015784  0.00000  0.000000  0.000000   \n",
       "3  0.000000  0.016699  0.018671  0.019890  0.00000  0.000000  0.000000   \n",
       "4  0.000000  0.007535  0.000000  0.006731  0.01508  0.000000  0.000000   \n",
       "5  0.000000  0.049726  0.000000  0.017768  0.00000  0.000000  0.000000   \n",
       "6  0.000000  0.015024  0.000000  0.013421  0.00000  0.015032  0.015032   \n",
       "7  0.000000  0.005944  0.009968  0.000000  0.00000  0.000000  0.000000   \n",
       "\n",
       "        一千多       一半     一口咬定  ...      鼻准骨        鼻子        鼻尖       齊物論  \\\n",
       "0  0.000000  0.00000  0.00000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.00000  0.00000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.00000  0.01768  ...  0.00000  0.033631  0.035359  0.000000   \n",
       "3  0.000000  0.00000  0.00000  ...  0.00000  0.007063  0.000000  0.000000   \n",
       "4  0.000000  0.01508  0.00000  ...  0.01508  0.047808  0.000000  0.000000   \n",
       "5  0.000000  0.00000  0.00000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "6  0.015032  0.00000  0.00000  ...  0.00000  0.009532  0.000000  0.000000   \n",
       "7  0.000000  0.00000  0.00000  ...  0.00000  0.000000  0.000000  0.011894   \n",
       "\n",
       "         齷齪       龍位       龍准       龍心       龍眉       龜山  \n",
       "0  0.028626  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "1  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "2  0.000000  0.01768  0.00000  0.01768  0.00000  0.01768  \n",
       "3  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "4  0.000000  0.00000  0.01508  0.00000  0.01508  0.00000  \n",
       "5  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "6  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "7  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "\n",
       "[8 rows x 4742 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(),columns=story_words)\n",
    "df_tfidf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:16pt\", align=\"center\">\n",
    "<b>Word Embedding</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\", align=\"center\">\n",
    "由於維基百科中提到，魯迅的作品在前期跟後期風格差異很大，以下將嘗試以<a href = \"https://zh.wikisource.org/wiki/%E5%90%B6%E5%96%8A\" target=\"_blank\">前期作品－《吶喊》</a>及<a href = \"https://zh.wikisource.org/wiki/%E6%95%85%E4%BA%8B%E6%96%B0%E7%B7%A8\" target=\"_blank\">後期作品－《故事新編》</a>訓練embedding後讓模型執行分類任務去看是否真的能透過文字區分兩個時期的作品。\n",
    "</p>\n",
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 1. 資料前處理</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨資料讀入\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shout = open(\"吶喊.txt\").read()\n",
    "shout_list = data_shout.split(\"-end-\")\n",
    "len(shout_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨斷句斷詞<br>\n",
    "&emsp;&ensp;因為兩本短篇小說集加起來只有22個章節，如果以章節作為單位，資料會太少，下面會在斷句過後進行embedding訓練。<br>\n",
    "&emsp;&ensp;因為魯迅的寫作風格比較符合常規，在該出現標點符號的地方都會出現標點符號，所以我們只要使用會放在句尾的標點符號作為斷句依據就可以了。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_to_sent(article_list):\n",
    "    ws = WS(\"D:\\In Bed\\研究所\\碩二下\\計畫\\data\")\n",
    "    sentence_list = []\n",
    "    for article in article_list:\n",
    "        sents = re.split(\"。|！|？|」|；\", article) # 斷句\n",
    "        for i in range(len(sents)):\n",
    "            sents[i] = re.sub(\"[^\\u4E00-\\u9FFF]+\", \" \", sents[i]) #過濾標點符號\n",
    "            if sents[i] != \" \":\n",
    "                clean_token = []\n",
    "                tokens = ws([sents[i]])\n",
    "                for token in tokens[0]:\n",
    "                    if token not in stopword_list and token != \" \":\n",
    "                        if \" \" in token:\n",
    "                            clean = re.sub(\" \", \"\", token)\n",
    "                            clean_token.append(clean)\n",
    "                        else:    \n",
    "                            clean_token.append(token)\n",
    "                sentence_list.append(\" \".join(clean_token))\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list_story = article_to_sent(story_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['女媧 醒來',\n",
       " '伊 似乎 夢 中 惊 醒 已經 記不清 做 夢',\n",
       " '懊惱 覺得 不足 覺得 太多',\n",
       " '煽動 和風 暖暾 伊 气力 吹 彌漫 宇宙里',\n",
       " '伊 揉 揉 眼睛',\n",
       " '粉紅 天空 中 曲曲折折 漂 許多 條 石綠色 浮雲 星 后面 忽 明 忽滅 眼',\n",
       " '天 血紅 雲彩里 光芒 四射 太陽 流動 金球包 荒古 熔岩 中',\n",
       " '卻 生鐵 冷 月亮',\n",
       " '伊并 理會 下去',\n",
       " '嫩綠 便是 換 葉 松柏 顯得 嬌嫩']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sentence_list_story))\n",
    "sentence_list_story[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c5227\\anaconda3\\lib\\site-packages\\ckiptagger\\model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['鄉下 跑到 京城 裏 一轉眼 已經 年',\n",
       " '其間 耳聞 目睹 所謂 國家 算起來',\n",
       " '心 裏 留 痕跡 倘 尋出 事 影響 說 增長 壞 脾氣 老實說 便是 教 天 天 看不起 人',\n",
       " '件 小 事 卻 意義 壞 脾氣 裏 拖開 使 至今 忘記',\n",
       " '民國 六年 冬天 北風 颳 正 猛 生計 關係 一早 路 走',\n",
       " '一路 遇 人 好容易 雇定 輛 人力車 教 拉到 門',\n",
       " '不一會 北風 小 路 浮塵 早已 刮淨 剩下 條 潔白 大道 車夫 跑',\n",
       " '剛近 門 忽而 車把 人 慢慢 倒',\n",
       " '跌倒 女人 花白 頭髮 衣服 破爛',\n",
       " '伊 馬路 突然 車 前 橫 截過來']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list_shout = article_to_sent(shout_list)\n",
    "print(len(sentence_list_shout))\n",
    "sentence_list_shout[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = sentence_list_story+sentence_list_shout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨標記生成<br>\n",
    "&emsp;&ensp;🎲 0：《故事新編》<br>\n",
    "&emsp;&ensp;🎲 1：《吶喊》<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label = []\n",
    "for i in range(len(sentence_list_shout)+len(sentence_list_story)):\n",
    "    if i < 2284:\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1)\n",
    "\n",
    "labels = np.array(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 2. 製作Embedding</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨取出所有word type\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10268\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentence_list)\n",
    "vocab_size = len(t.word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨Hash 編代號\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6914, 971]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "hashed_sent = [one_hot(d, vocab_size) for d in sentence_list]\n",
    "print(hashed_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(d.split()) for d in sentence_list])\n",
    "print(max_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨Padding\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6914  971    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "padded_sent = pad_sequences(hashed_sent, maxlen=max_length, padding='post')\n",
    "print(padded_sent[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 3. 模型訓練及評估</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨設定種子確保結果的再現性\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨模型初始化\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_16 (Embedding)    (None, 42, 10)            102680    \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 420)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 421       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103,101\n",
      "Trainable params: 103,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨模型訓練+結果評估\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.431754\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_sent, labels, epochs=25, verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(padded_sent, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('Loss: %d' % loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 4. 視覺化 embedding layer</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨檔案寫出<br>\n",
    "&emsp;&ensp;分別寫出權重檔案跟word type檔案以便移至<a href=\"https://projector.tensorflow.org/\" target=\"blank_\">Tensorflow的Embedding Projector</a>使用。<br>\n",
    "&emsp;&ensp;因為沒辦法匯出投影結果，下面用影片展示。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "words = list(t.word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "weight_vec = io.open(\"weight_vec.tsv\", \"w\", encoding=\"utf-8\")\n",
    "word_arr = io.open(\"words.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    word_vec = weights[i]\n",
    "    word_arr.write(words[i]+\"\\n\")\n",
    "    weight_vec.write(\"\\t\".join([str(x) for x in word_vec]) + \"\\n\")\n",
    "weight_vec.close()\n",
    "word_arr.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video src=\"https://drive.google.com/uc?export=download&id=1uTA_9VlA84HtYx4YoH89fWLvWAr5W8oF\"\n",
    "       width=\"1080\" \n",
    "       autoplay=\"true\" \n",
    "       controls=\"true\" >\n",
    "</video>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:16pt\", align=\"center\">\n",
    "<b>Word Embedding-2</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\", align=\"center\">\n",
    "下面依照正常的模型訓練程序將資料分為80%的訓練集跟20%的測試集訓練模型，並測試<code>shuffle</code>參數對模型訓練結果的影響。\n",
    "</p>\n",
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 1. 資料前處理</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨ 區分訓練集及測試集\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "shuffled_feature, shuffled_label = shuffle(padded_sent, labels, random_state = 168)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(shuffled_feature, shuffled_label, test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨ 模型訓練及結果評估\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.186833\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "new_model = Sequential()\n",
    "new_model.add(Embedding(vocab_size, 10, input_length=max_length)) \n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "new_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "new_model.fit(feature_train, np.array(label_train), epochs=50, verbose=0, shuffle=True, initial_epoch=0)\n",
    "\n",
    "new_loss, new_accuracy = new_model.evaluate(feature_test, np.array(label_test), verbose=0)\n",
    "\n",
    "print('Accuracy: %f' % (new_accuracy*100))\n",
    "print('Loss: %d' % new_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.057117\n",
      "Loss: 1\n"
     ]
    }
   ],
   "source": [
    "new_model.fit(feature_train, np.array(label_train), epochs=50, verbose=0, shuffle=False, initial_epoch=0)\n",
    "\n",
    "new_loss, new_accuracy = new_model.evaluate(feature_test, np.array(label_test), verbose=0)\n",
    "print('Accuracy: %f' % (new_accuracy*100))\n",
    "print('Loss: %d' % new_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\", style = \"font-size:16pt\">\n",
    "    🧐<b>討論</b>🧐<br>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "   &emsp;&emsp;這邊首先可以看到，不管<code>shuffle</code>調成怎樣，結果都比剛剛用全部資料訓練完直接評估的差。這基本上可以說是很理所當然的事。因為用來訓練的資料完全跟拿來預測的資料相同，模型基本上已經掌握整個資料集的分布了，當然就可以表現得不錯。下面則是因為用來訓練的資料變少了，所以沒辦法達到那麼好的效果。但我們可以看到整體表現還是不差，證明我們確實可以透過word embedding掌握魯迅不同時期作品的差異。<br>\n",
    "   &emsp;&emsp;接下來關於<code>shuffle</code>的部分，因為它設定的是要不要讓模型在每個epoch都把訓練集資料的順序重整，我們可以看到資料的排序多多少少還是會影響到訓練結果，所以設定每次會重整資料的模型可以透過不斷重整來達到比較好也比較有公信力的成效。\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:16pt\", align=\"center\">\n",
    "<b>GloVe</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\", align=\"center\">\n",
    "這邊跟上上週一樣使用從<a href = \"https://www.kaggle.com/datasets/niraliivaghani/flipkart-product-customer-reviews-dataset\" target=\"_blank\">Kaggle</a>取得的<a href = \"https://www.flipkart.com/\" target = \"_blank\">線上購物網站－Flipkart</a>上，消費者對商品的評論資料來實作情感分析並比較結果差異。\n",
    "</p>\n",
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 1. 資料前處理</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨ 資料讀入\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 180376 entries, 0 to 180375\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   product_name   180376 non-null  object\n",
      " 1   product_price  180376 non-null  int64 \n",
      " 2   Rate           180376 non-null  int64 \n",
      " 3   Review         180376 non-null  object\n",
      " 4   Summary        180376 non-null  object\n",
      " 5   Sentiment      180376 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 8.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"flipkart_sentiment.csv\")\n",
    "clean_data = data.dropna(axis=0).reset_index(drop = True)\n",
    "clean_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨資料簡述<br>\n",
    "&emsp;&ensp;該dataset中包含下列六個column：<br>\n",
    "&emsp;&ensp;🎲 產品名稱<br>\n",
    "&emsp;&ensp;🎲 產品價格<br>\n",
    "&emsp;&ensp;🎲 消費者評分：消費者對商品的滿意度評分（1~5分）<br>\n",
    "&emsp;&ensp;🎲 固化評語：消費者根據網站提供模板選取針對產品的評價<br>\n",
    "&emsp;&ensp;🎲 消費者評論：消費者針對商品親自撰寫的評價<br>\n",
    "&emsp;&ensp;🎲 評論正負向：資料提供者根據消費者評論給予的分類，包含positive, negative, neutral三種<br>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨去除分類為neutral的資料<br>\n",
    "&emsp;&ensp;因為下面要做二元分類的任務，這邊直接把分類為neutral資料拿掉，專注做正負向情感分析。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = []\n",
    "index_counter = 0\n",
    "\n",
    "for label in clean_data[\"Sentiment\"]:\n",
    "  if label != \"positive\" and label != \"negative\":\n",
    "    drop_list.append(index_counter)\n",
    "  index_counter += 1\n",
    "\n",
    "clean_data = clean_data.drop(index = drop_list, axis = 0)\n",
    "clean_data = clean_data.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨分離評論跟答案<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = list(clean_data[\"Summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label = clean_data[[\"Sentiment\"]].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 2. 準備 embeddings</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;為了節省內存，下面選擇GloVe提供最小的embedding資料來用。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "embeddings_index = dict()\n",
    "\n",
    "with open('glove.6B/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 3. padding</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "t_comment = Tokenizer()\n",
    "t_comment.fit_on_texts(comments)\n",
    "\n",
    "comment_vocab_size = len(t.word_index)\n",
    "hashed_comment = [one_hot(d, comment_vocab_size) for d in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10268\n",
      "[2852, 1954, 5053, 4089, 6852, 4867, 1112, 7124, 4421, 4458, 7859, 3779, 4867, 6264, 3803, 2690]\n"
     ]
    }
   ],
   "source": [
    "print(comment_vocab_size)\n",
    "print(hashed_comment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "max_length_eng = max([len(d.split()) for d in comments])\n",
    "print(max_length_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2852 1954 5053 4089 6852 4867 1112 7124 4421 4458 7859 3779 4867 6264\n",
      " 3803 2690    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "padded_comment = pad_sequences(hashed_comment, maxlen=max_length_eng, padding='post')\n",
    "print(padded_comment[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 4. 模型訓練</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "✨區分訓練集跟測試集<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_eng_train, feature_eng_test, label_eng_train, label_eng_test = train_test_split(padded_comment, label, test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨初始化模型<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 108, 100)          1026800   \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 10800)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 10801     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,037,601\n",
      "Trainable params: 10,801\n",
      "Non-trainable params: 1,026,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "eng_model = Sequential()\n",
    "e = Embedding(comment_vocab_size, 100, weights=[embedding_matrix], input_length=108, trainable=False)\n",
    "eng_model.add(e)\n",
    "eng_model.add(Flatten())\n",
    "eng_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "eng_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(eng_model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "✨模型訓練及評估<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.900801\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "eng_model.fit(feature_eng_train, np.array(label_eng_train), epochs=50, verbose=0)\n",
    "\n",
    "eng_loss, eng_accuracy = eng_model.evaluate(feature_eng_test, np.array(label_eng_test), verbose=0)\n",
    "print('Accuracy: %f' % (eng_accuracy*100))\n",
    "print('Loss: %d' % eng_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\", style = \"font-size:16pt\">\n",
    "    🧐<b>討論</b>🧐<br>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "   &emsp;&emsp;從上面我們可以看到模型訓練的結果達到0.85，其實算是還不錯。但如果跟當初使用<code>nltk.vader</code>計算的情感分數訓練出來的Naive Bayes模型（accuracy: 0.98）相比，就有點差強人意了。推測會這樣主要的原因有兩個。第一個是<code>GloVe</code>提供的embedding是他們透過各種資料預訓練完的，並不特別針對特定的分類任務設計。因此雖然能夠有一定的水準，卻比較難達到像<code>vadar</code>這樣專門為情感分析設計的套件產出的特徵所訓練出的模型那麼好的成效。第二個原因是這次挑選的embedding資料不夠大。訓練資料越多能達到越好的效果是機器學習的法則，所以不排除運用更大的embedding資料可以達到跟Naive Bayes並駕齊驅結果的可能性。\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
