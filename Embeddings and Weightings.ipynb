{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ä½™ç›ˆè““ï¼›èªç¢©äºŒï¼›110555009**\\\n",
    "**111-2 Computational Linguistics**\n",
    "<p align=\"center\", style = \"font-size:18pt\">\n",
    "<b>4th Assignment<br>\n",
    "Embeddings and Weightings</b>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:16pt\", align=\"center\">\n",
    "<b>TF-IDF</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\", align=\"center\">\n",
    "ä¸‹é¢ç”¨é­¯è¿…çš„çŸ­ç¯‡å°èªªé›†<a href = \"https://zh.wikisource.org/wiki/%E6%95%85%E4%BA%8B%E6%96%B0%E7%B7%A8\" target=\"_blank\">ã€Šæ•…äº‹æ–°ç·¨ã€‹</a>ä¸­çš„8å€‹ç« ç¯€ä¾†è¨ˆç®—TF-IDFã€‚\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "âœ¨è³‡æ–™è®€å…¥\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'å¥³åª§å¿½ç„¶é†’ä¾†äº†ã€‚\\n\\nä¼Šä¼¼ä¹æ˜¯å¾å¤¢ä¸­æƒŠé†’çš„ï¼Œç„¶è€Œå·²ç¶“è¨˜ä¸æ¸…åšäº†ä»€éº¼å¤¢ï¼›åªæ˜¯å¾ˆæ‡Šæƒ±ï¼Œè¦ºå¾—æœ‰ä»€éº¼ä¸è¶³ï¼Œåˆè¦º'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_story = open(\"æ•…äº‹æ–°ç·¨.txt\").read()\n",
    "data_story[0:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨ç« ç¯€åˆ†å‰²\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'å¥³åª§å¿½ç„¶é†’ä¾†äº†ã€‚\\n\\nä¼Šä¼¼ä¹æ˜¯å¾å¤¢ä¸­æƒŠé†’çš„ï¼Œç„¶è€Œå·²ç¶“è¨˜ä¸æ¸…åšäº†ä»€éº¼å¤¢ï¼›åªæ˜¯å¾ˆæ‡Šæƒ±ï¼Œè¦ºå¾—æœ‰ä»€éº¼ä¸è¶³ï¼Œåˆè¦º'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_list = data_story.split(\"-end-\")\n",
    "story_list[0][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(story_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨æ–·è©\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = open(\"Chinese_stopwords.txt\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c5227\\anaconda3\\lib\\site-packages\\ckiptagger\\model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å¥³åª§ é†’ä¾† ã€‚ \\n\\nä¼Š ä¼¼ä¹ å¤¢ ä¸­ æƒŠ é†’ å·²ç¶“ è¨˜ä¸æ¸… åš å¤¢ ï¼› æ‡Šæƒ± è¦ºå¾— ä¸è¶³ è¦ºå¾— å¤ªå¤š'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckiptagger import WS\n",
    "\n",
    "ws = WS(\"D:\\In Bed\\ç ”ç©¶æ‰€\\ç¢©äºŒä¸‹\\è¨ˆç•«\\data\")\n",
    "story_corpus = []\n",
    "\n",
    "for story in story_list:\n",
    "    tokens = ws([story])\n",
    "    clean_tokens=[]\n",
    "    for i in tokens[0]:\n",
    "        if i not in stopword_list:\n",
    "            clean_tokens.append(i)\n",
    "    story_corpus.append(\" \".join(clean_tokens))\n",
    "\n",
    "story_corpus[0][0:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨è³‡æ–™æ¸…æ´—\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in range(len(story_corpus)):\n",
    "    story_corpus[i] = re.sub(\"\\n+\", \"\", story_corpus[i])\n",
    "    story_corpus[i] = re.sub(\"[A-z]+ \", \" \", story_corpus[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨ç”ŸæˆTF-IDF\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ä€¹çœ¼', 'ä¸€ä¸‹', 'ä¸€ä¸‹å­', 'ä¸€äº›', 'ä¸€å€‹å€‹', 'ä¸€å…±', 'ä¸€åƒ', 'ä¸€åƒå¤š', 'ä¸€åŠ', 'ä¸€å£å’¬å®š',\n",
       "       'ä¸€å£é¢', 'ä¸€åŒ', 'ä¸€å‘', 'ä¸€å‘³', 'ä¸€åœ˜ç³Ÿ', 'ä¸€å¤«', 'ä¸€å­—', 'ä¸€å®š', 'ä¸€å¹³', 'ä¸€å¹´åˆ°é ­',\n",
       "       'ä¸€å¾‹', 'ä¸€æ‰‹', 'ä¸€æ­åˆç™¼', 'ä¸€æ—©', 'ä¸€æ™‚', 'ä¸€æœƒ', 'ä¸€æœˆ', 'ä¸€æœ›', 'ä¸€æ®µè½', 'ä¸€æºœç…™',\n",
       "       'ä¸€ç„¡æ‰€æœ‰', 'ä¸€ç†”åŒ–', 'ä¸€ç™¾å¤š', 'ä¸€ç™¾é›¶ä¸‰å››', 'ä¸€ç›´', 'ä¸€ç¬æ¯', 'ä¸€ç¨®', 'ä¸€çµ²ä¸æ›', 'ä¸€ç¶“',\n",
       "       'ä¸€è²ä¸éŸ¿', 'ä¸€è‡´', 'ä¸€è‰²', 'ä¸€è¬', 'ä¸€è§’', 'ä¸€èµ·', 'ä¸€è·¯', 'ä¸€è½‰çœ¼é–“', 'ä¸€é€š', 'ä¸€é™£',\n",
       "       'ä¸€é¢'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(story_corpus)\n",
    "story_words = tfidf_vectorizer.get_feature_names_out()\n",
    "story_words[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ä€¹çœ¼</th>\n",
       "      <th>ä¸€ä¸‹</th>\n",
       "      <th>ä¸€ä¸‹å­</th>\n",
       "      <th>ä¸€äº›</th>\n",
       "      <th>ä¸€å€‹å€‹</th>\n",
       "      <th>ä¸€å…±</th>\n",
       "      <th>ä¸€åƒ</th>\n",
       "      <th>ä¸€åƒå¤š</th>\n",
       "      <th>ä¸€åŠ</th>\n",
       "      <th>ä¸€å£å’¬å®š</th>\n",
       "      <th>...</th>\n",
       "      <th>é¼»å‡†éª¨</th>\n",
       "      <th>é¼»å­</th>\n",
       "      <th>é¼»å°–</th>\n",
       "      <th>é½Šç‰©è«–</th>\n",
       "      <th>é½·é½ª</th>\n",
       "      <th>é¾ä½</th>\n",
       "      <th>é¾å‡†</th>\n",
       "      <th>é¾å¿ƒ</th>\n",
       "      <th>é¾çœ‰</th>\n",
       "      <th>é¾œå±±</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025556</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028626</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015784</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.033631</td>\n",
       "      <td>0.035359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01768</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01768</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016699</td>\n",
       "      <td>0.018671</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.047808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01508</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017768</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 4742 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ä€¹çœ¼        ä¸€ä¸‹       ä¸€ä¸‹å­        ä¸€äº›      ä¸€å€‹å€‹        ä¸€å…±        ä¸€åƒ  \\\n",
       "0  0.028626  0.000000  0.000000  0.025556  0.00000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.022074  0.00000  0.000000  0.000000   \n",
       "2  0.000000  0.035339  0.000000  0.015784  0.00000  0.000000  0.000000   \n",
       "3  0.000000  0.016699  0.018671  0.019890  0.00000  0.000000  0.000000   \n",
       "4  0.000000  0.007535  0.000000  0.006731  0.01508  0.000000  0.000000   \n",
       "5  0.000000  0.049726  0.000000  0.017768  0.00000  0.000000  0.000000   \n",
       "6  0.000000  0.015024  0.000000  0.013421  0.00000  0.015032  0.015032   \n",
       "7  0.000000  0.005944  0.009968  0.000000  0.00000  0.000000  0.000000   \n",
       "\n",
       "        ä¸€åƒå¤š       ä¸€åŠ     ä¸€å£å’¬å®š  ...      é¼»å‡†éª¨        é¼»å­        é¼»å°–       é½Šç‰©è«–  \\\n",
       "0  0.000000  0.00000  0.00000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.00000  0.00000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.00000  0.01768  ...  0.00000  0.033631  0.035359  0.000000   \n",
       "3  0.000000  0.00000  0.00000  ...  0.00000  0.007063  0.000000  0.000000   \n",
       "4  0.000000  0.01508  0.00000  ...  0.01508  0.047808  0.000000  0.000000   \n",
       "5  0.000000  0.00000  0.00000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "6  0.015032  0.00000  0.00000  ...  0.00000  0.009532  0.000000  0.000000   \n",
       "7  0.000000  0.00000  0.00000  ...  0.00000  0.000000  0.000000  0.011894   \n",
       "\n",
       "         é½·é½ª       é¾ä½       é¾å‡†       é¾å¿ƒ       é¾çœ‰       é¾œå±±  \n",
       "0  0.028626  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "1  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "2  0.000000  0.01768  0.00000  0.01768  0.00000  0.01768  \n",
       "3  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "4  0.000000  0.00000  0.01508  0.00000  0.01508  0.00000  \n",
       "5  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "6  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "7  0.000000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "\n",
       "[8 rows x 4742 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(),columns=story_words)\n",
    "df_tfidf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:16pt\", align=\"center\">\n",
    "<b>Word Embedding</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\", align=\"center\">\n",
    "ç”±æ–¼ç¶­åŸºç™¾ç§‘ä¸­æåˆ°ï¼Œé­¯è¿…çš„ä½œå“åœ¨å‰æœŸè·Ÿå¾ŒæœŸé¢¨æ ¼å·®ç•°å¾ˆå¤§ï¼Œä»¥ä¸‹å°‡å˜—è©¦ä»¥<a href = \"https://zh.wikisource.org/wiki/%E5%90%B6%E5%96%8A\" target=\"_blank\">å‰æœŸä½œå“ï¼ã€Šå¶å–Šã€‹</a>åŠ<a href = \"https://zh.wikisource.org/wiki/%E6%95%85%E4%BA%8B%E6%96%B0%E7%B7%A8\" target=\"_blank\">å¾ŒæœŸä½œå“ï¼ã€Šæ•…äº‹æ–°ç·¨ã€‹</a>è¨“ç·´embeddingå¾Œè®“æ¨¡å‹åŸ·è¡Œåˆ†é¡ä»»å‹™å»çœ‹æ˜¯å¦çœŸçš„èƒ½é€éæ–‡å­—å€åˆ†å…©å€‹æ™‚æœŸçš„ä½œå“ã€‚\n",
    "</p>\n",
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 1. è³‡æ–™å‰è™•ç†</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "âœ¨è³‡æ–™è®€å…¥\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shout = open(\"å¶å–Š.txt\").read()\n",
    "shout_list = data_shout.split(\"-end-\")\n",
    "len(shout_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨æ–·å¥æ–·è©<br>\n",
    "&emsp;&ensp;å› ç‚ºå…©æœ¬çŸ­ç¯‡å°èªªé›†åŠ èµ·ä¾†åªæœ‰22å€‹ç« ç¯€ï¼Œå¦‚æœä»¥ç« ç¯€ä½œç‚ºå–®ä½ï¼Œè³‡æ–™æœƒå¤ªå°‘ï¼Œä¸‹é¢æœƒåœ¨æ–·å¥éå¾Œé€²è¡Œembeddingè¨“ç·´ã€‚<br>\n",
    "&emsp;&ensp;å› ç‚ºé­¯è¿…çš„å¯«ä½œé¢¨æ ¼æ¯”è¼ƒç¬¦åˆå¸¸è¦ï¼Œåœ¨è©²å‡ºç¾æ¨™é»ç¬¦è™Ÿçš„åœ°æ–¹éƒ½æœƒå‡ºç¾æ¨™é»ç¬¦è™Ÿï¼Œæ‰€ä»¥æˆ‘å€‘åªè¦ä½¿ç”¨æœƒæ”¾åœ¨å¥å°¾çš„æ¨™é»ç¬¦è™Ÿä½œç‚ºæ–·å¥ä¾æ“šå°±å¯ä»¥äº†ã€‚\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_to_sent(article_list):\n",
    "    ws = WS(\"D:\\In Bed\\ç ”ç©¶æ‰€\\ç¢©äºŒä¸‹\\è¨ˆç•«\\data\")\n",
    "    sentence_list = []\n",
    "    for article in article_list:\n",
    "        sents = re.split(\"ã€‚|ï¼|ï¼Ÿ|ã€|ï¼›\", article) # æ–·å¥\n",
    "        for i in range(len(sents)):\n",
    "            sents[i] = re.sub(\"[^\\u4E00-\\u9FFF]+\", \" \", sents[i]) #éæ¿¾æ¨™é»ç¬¦è™Ÿ\n",
    "            if sents[i] != \" \":\n",
    "                clean_token = []\n",
    "                tokens = ws([sents[i]])\n",
    "                for token in tokens[0]:\n",
    "                    if token not in stopword_list and token != \" \":\n",
    "                        if \" \" in token:\n",
    "                            clean = re.sub(\" \", \"\", token)\n",
    "                            clean_token.append(clean)\n",
    "                        else:    \n",
    "                            clean_token.append(token)\n",
    "                sentence_list.append(\" \".join(clean_token))\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list_story = article_to_sent(story_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['å¥³åª§ é†’ä¾†',\n",
       " 'ä¼Š ä¼¼ä¹ å¤¢ ä¸­ æƒŠ é†’ å·²ç¶“ è¨˜ä¸æ¸… åš å¤¢',\n",
       " 'æ‡Šæƒ± è¦ºå¾— ä¸è¶³ è¦ºå¾— å¤ªå¤š',\n",
       " 'ç…½å‹• å’Œé¢¨ æš–æš¾ ä¼Š æ°”åŠ› å¹ å½Œæ¼« å®‡å®™é‡Œ',\n",
       " 'ä¼Š æ‰ æ‰ çœ¼ç›',\n",
       " 'ç²‰ç´… å¤©ç©º ä¸­ æ›²æ›²æŠ˜æŠ˜ æ¼‚ è¨±å¤š æ¢ çŸ³ç¶ è‰² æµ®é›² æ˜Ÿ åé¢ å¿½ æ˜ å¿½æ»… çœ¼',\n",
       " 'å¤© è¡€ç´… é›²å½©é‡Œ å…‰èŠ’ å››å°„ å¤ªé™½ æµå‹• é‡‘çƒåŒ… è’å¤ ç†”å²© ä¸­',\n",
       " 'å» ç”Ÿéµ å†· æœˆäº®',\n",
       " 'ä¼Šå¹¶ ç†æœƒ ä¸‹å»',\n",
       " 'å«©ç¶  ä¾¿æ˜¯ æ› è‘‰ æ¾æŸ é¡¯å¾— å¬Œå«©']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sentence_list_story))\n",
    "sentence_list_story[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\c5227\\anaconda3\\lib\\site-packages\\ckiptagger\\model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['é„‰ä¸‹ è·‘åˆ° äº¬åŸ è£ ä¸€è½‰çœ¼ å·²ç¶“ å¹´',\n",
       " 'å…¶é–“ è€³è ç›®ç¹ æ‰€è¬‚ åœ‹å®¶ ç®—èµ·ä¾†',\n",
       " 'å¿ƒ è£ ç•™ ç—•è·¡ å€˜ å°‹å‡º äº‹ å½±éŸ¿ èªª å¢é•· å£ è„¾æ°£ è€å¯¦èªª ä¾¿æ˜¯ æ•™ å¤© å¤© çœ‹ä¸èµ· äºº',\n",
       " 'ä»¶ å° äº‹ å» æ„ç¾© å£ è„¾æ°£ è£ æ‹–é–‹ ä½¿ è‡³ä»Š å¿˜è¨˜',\n",
       " 'æ°‘åœ‹ å…­å¹´ å†¬å¤© åŒ—é¢¨ é¢³ æ­£ çŒ› ç”Ÿè¨ˆ é—œä¿‚ ä¸€æ—© è·¯ èµ°',\n",
       " 'ä¸€è·¯ é‡ äºº å¥½å®¹æ˜“ é›‡å®š è¼› äººåŠ›è»Š æ•™ æ‹‰åˆ° é–€',\n",
       " 'ä¸ä¸€æœƒ åŒ—é¢¨ å° è·¯ æµ®å¡µ æ—©å·² åˆ®æ·¨ å‰©ä¸‹ æ¢ æ½”ç™½ å¤§é“ è»Šå¤« è·‘',\n",
       " 'å‰›è¿‘ é–€ å¿½è€Œ è»ŠæŠŠ äºº æ…¢æ…¢ å€’',\n",
       " 'è·Œå€’ å¥³äºº èŠ±ç™½ é ­é«® è¡£æœ ç ´çˆ›',\n",
       " 'ä¼Š é¦¬è·¯ çªç„¶ è»Š å‰ æ©« æˆªéä¾†']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list_shout = article_to_sent(shout_list)\n",
    "print(len(sentence_list_shout))\n",
    "sentence_list_shout[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = sentence_list_story+sentence_list_shout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨æ¨™è¨˜ç”Ÿæˆ<br>\n",
    "&emsp;&ensp;ğŸ² 0ï¼šã€Šæ•…äº‹æ–°ç·¨ã€‹<br>\n",
    "&emsp;&ensp;ğŸ² 1ï¼šã€Šå¶å–Šã€‹<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label = []\n",
    "for i in range(len(sentence_list_shout)+len(sentence_list_story)):\n",
    "    if i < 2284:\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1)\n",
    "\n",
    "labels = np.array(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 2. è£½ä½œEmbedding</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "âœ¨å–å‡ºæ‰€æœ‰word type\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10268\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentence_list)\n",
    "vocab_size = len(t.word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨Hash ç·¨ä»£è™Ÿ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6914, 971]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "hashed_sent = [one_hot(d, vocab_size) for d in sentence_list]\n",
    "print(hashed_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(d.split()) for d in sentence_list])\n",
    "print(max_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨Padding\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6914  971    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "padded_sent = pad_sequences(hashed_sent, maxlen=max_length, padding='post')\n",
    "print(padded_sent[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 3. æ¨¡å‹è¨“ç·´åŠè©•ä¼°</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "âœ¨è¨­å®šç¨®å­ç¢ºä¿çµæœçš„å†ç¾æ€§\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨æ¨¡å‹åˆå§‹åŒ–\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_16 (Embedding)    (None, 42, 10)            102680    \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 420)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 421       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103,101\n",
      "Trainable params: 103,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨æ¨¡å‹è¨“ç·´+çµæœè©•ä¼°\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.431754\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_sent, labels, epochs=25, verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(padded_sent, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('Loss: %d' % loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 4. è¦–è¦ºåŒ– embedding layer</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "âœ¨æª”æ¡ˆå¯«å‡º<br>\n",
    "&emsp;&ensp;åˆ†åˆ¥å¯«å‡ºæ¬Šé‡æª”æ¡ˆè·Ÿword typeæª”æ¡ˆä»¥ä¾¿ç§»è‡³<a href=\"https://projector.tensorflow.org/\" target=\"blank_\">Tensorflowçš„Embedding Projector</a>ä½¿ç”¨ã€‚<br>\n",
    "&emsp;&ensp;å› ç‚ºæ²’è¾¦æ³•åŒ¯å‡ºæŠ•å½±çµæœï¼Œä¸‹é¢ç”¨å½±ç‰‡å±•ç¤ºã€‚\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "words = list(t.word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "weight_vec = io.open(\"weight_vec.tsv\", \"w\", encoding=\"utf-8\")\n",
    "word_arr = io.open(\"words.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    word_vec = weights[i]\n",
    "    word_arr.write(words[i]+\"\\n\")\n",
    "    weight_vec.write(\"\\t\".join([str(x) for x in word_vec]) + \"\\n\")\n",
    "weight_vec.close()\n",
    "word_arr.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video src=\"https://drive.google.com/uc?export=download&id=1uTA_9VlA84HtYx4YoH89fWLvWAr5W8oF\"\n",
    "       width=\"1080\" \n",
    "       autoplay=\"true\" \n",
    "       controls=\"true\" >\n",
    "</video>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:16pt\", align=\"center\">\n",
    "<b>Word Embedding-2</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\", align=\"center\">\n",
    "ä¸‹é¢ä¾ç…§æ­£å¸¸çš„æ¨¡å‹è¨“ç·´ç¨‹åºå°‡è³‡æ–™åˆ†ç‚º80%çš„è¨“ç·´é›†è·Ÿ20%çš„æ¸¬è©¦é›†è¨“ç·´æ¨¡å‹ï¼Œä¸¦æ¸¬è©¦<code>shuffle</code>åƒæ•¸å°æ¨¡å‹è¨“ç·´çµæœçš„å½±éŸ¿ã€‚\n",
    "</p>\n",
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 1. è³‡æ–™å‰è™•ç†</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "âœ¨ å€åˆ†è¨“ç·´é›†åŠæ¸¬è©¦é›†\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "shuffled_feature, shuffled_label = shuffle(padded_sent, labels, random_state = 168)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(shuffled_feature, shuffled_label, test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨ æ¨¡å‹è¨“ç·´åŠçµæœè©•ä¼°\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.186833\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "new_model = Sequential()\n",
    "new_model.add(Embedding(vocab_size, 10, input_length=max_length)) \n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "new_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "new_model.fit(feature_train, np.array(label_train), epochs=50, verbose=0, shuffle=True, initial_epoch=0)\n",
    "\n",
    "new_loss, new_accuracy = new_model.evaluate(feature_test, np.array(label_test), verbose=0)\n",
    "\n",
    "print('Accuracy: %f' % (new_accuracy*100))\n",
    "print('Loss: %d' % new_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.057117\n",
      "Loss: 1\n"
     ]
    }
   ],
   "source": [
    "new_model.fit(feature_train, np.array(label_train), epochs=50, verbose=0, shuffle=False, initial_epoch=0)\n",
    "\n",
    "new_loss, new_accuracy = new_model.evaluate(feature_test, np.array(label_test), verbose=0)\n",
    "print('Accuracy: %f' % (new_accuracy*100))\n",
    "print('Loss: %d' % new_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\", style = \"font-size:16pt\">\n",
    "    ğŸ§<b>è¨è«–</b>ğŸ§<br>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "   &emsp;&emsp;é€™é‚Šé¦–å…ˆå¯ä»¥çœ‹åˆ°ï¼Œä¸ç®¡<code>shuffle</code>èª¿æˆæ€æ¨£ï¼Œçµæœéƒ½æ¯”å‰›å‰›ç”¨å…¨éƒ¨è³‡æ–™è¨“ç·´å®Œç›´æ¥è©•ä¼°çš„å·®ã€‚é€™åŸºæœ¬ä¸Šå¯ä»¥èªªæ˜¯å¾ˆç†æ‰€ç•¶ç„¶çš„äº‹ã€‚å› ç‚ºç”¨ä¾†è¨“ç·´çš„è³‡æ–™å®Œå…¨è·Ÿæ‹¿ä¾†é æ¸¬çš„è³‡æ–™ç›¸åŒï¼Œæ¨¡å‹åŸºæœ¬ä¸Šå·²ç¶“æŒæ¡æ•´å€‹è³‡æ–™é›†çš„åˆ†å¸ƒäº†ï¼Œç•¶ç„¶å°±å¯ä»¥è¡¨ç¾å¾—ä¸éŒ¯ã€‚ä¸‹é¢å‰‡æ˜¯å› ç‚ºç”¨ä¾†è¨“ç·´çš„è³‡æ–™è®Šå°‘äº†ï¼Œæ‰€ä»¥æ²’è¾¦æ³•é”åˆ°é‚£éº¼å¥½çš„æ•ˆæœã€‚ä½†æˆ‘å€‘å¯ä»¥çœ‹åˆ°æ•´é«”è¡¨ç¾é‚„æ˜¯ä¸å·®ï¼Œè­‰æ˜æˆ‘å€‘ç¢ºå¯¦å¯ä»¥é€éword embeddingæŒæ¡é­¯è¿…ä¸åŒæ™‚æœŸä½œå“çš„å·®ç•°ã€‚<br>\n",
    "   &emsp;&emsp;æ¥ä¸‹ä¾†é—œæ–¼<code>shuffle</code>çš„éƒ¨åˆ†ï¼Œå› ç‚ºå®ƒè¨­å®šçš„æ˜¯è¦ä¸è¦è®“æ¨¡å‹åœ¨æ¯å€‹epochéƒ½æŠŠè¨“ç·´é›†è³‡æ–™çš„é †åºé‡æ•´ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°è³‡æ–™çš„æ’åºå¤šå¤šå°‘å°‘é‚„æ˜¯æœƒå½±éŸ¿åˆ°è¨“ç·´çµæœï¼Œæ‰€ä»¥è¨­å®šæ¯æ¬¡æœƒé‡æ•´è³‡æ–™çš„æ¨¡å‹å¯ä»¥é€éä¸æ–·é‡æ•´ä¾†é”åˆ°æ¯”è¼ƒå¥½ä¹Ÿæ¯”è¼ƒæœ‰å…¬ä¿¡åŠ›çš„æˆæ•ˆã€‚\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:16pt\", align=\"center\">\n",
    "<b>GloVe</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\", align=\"center\">\n",
    "é€™é‚Šè·Ÿä¸Šä¸Šé€±ä¸€æ¨£ä½¿ç”¨å¾<a href = \"https://www.kaggle.com/datasets/niraliivaghani/flipkart-product-customer-reviews-dataset\" target=\"_blank\">Kaggle</a>å–å¾—çš„<a href = \"https://www.flipkart.com/\" target = \"_blank\">ç·šä¸Šè³¼ç‰©ç¶²ç«™ï¼Flipkart</a>ä¸Šï¼Œæ¶ˆè²»è€…å°å•†å“çš„è©•è«–è³‡æ–™ä¾†å¯¦ä½œæƒ…æ„Ÿåˆ†æä¸¦æ¯”è¼ƒçµæœå·®ç•°ã€‚\n",
    "</p>\n",
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 1. è³‡æ–™å‰è™•ç†</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "âœ¨ è³‡æ–™è®€å…¥\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 180376 entries, 0 to 180375\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   product_name   180376 non-null  object\n",
      " 1   product_price  180376 non-null  int64 \n",
      " 2   Rate           180376 non-null  int64 \n",
      " 3   Review         180376 non-null  object\n",
      " 4   Summary        180376 non-null  object\n",
      " 5   Sentiment      180376 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 8.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"flipkart_sentiment.csv\")\n",
    "clean_data = data.dropna(axis=0).reset_index(drop = True)\n",
    "clean_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨è³‡æ–™ç°¡è¿°<br>\n",
    "&emsp;&ensp;è©²datasetä¸­åŒ…å«ä¸‹åˆ—å…­å€‹columnï¼š<br>\n",
    "&emsp;&ensp;ğŸ² ç”¢å“åç¨±<br>\n",
    "&emsp;&ensp;ğŸ² ç”¢å“åƒ¹æ ¼<br>\n",
    "&emsp;&ensp;ğŸ² æ¶ˆè²»è€…è©•åˆ†ï¼šæ¶ˆè²»è€…å°å•†å“çš„æ»¿æ„åº¦è©•åˆ†ï¼ˆ1~5åˆ†ï¼‰<br>\n",
    "&emsp;&ensp;ğŸ² å›ºåŒ–è©•èªï¼šæ¶ˆè²»è€…æ ¹æ“šç¶²ç«™æä¾›æ¨¡æ¿é¸å–é‡å°ç”¢å“çš„è©•åƒ¹<br>\n",
    "&emsp;&ensp;ğŸ² æ¶ˆè²»è€…è©•è«–ï¼šæ¶ˆè²»è€…é‡å°å•†å“è¦ªè‡ªæ’°å¯«çš„è©•åƒ¹<br>\n",
    "&emsp;&ensp;ğŸ² è©•è«–æ­£è² å‘ï¼šè³‡æ–™æä¾›è€…æ ¹æ“šæ¶ˆè²»è€…è©•è«–çµ¦äºˆçš„åˆ†é¡ï¼ŒåŒ…å«positive, negative, neutralä¸‰ç¨®<br>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨å»é™¤åˆ†é¡ç‚ºneutralçš„è³‡æ–™<br>\n",
    "&emsp;&ensp;å› ç‚ºä¸‹é¢è¦åšäºŒå…ƒåˆ†é¡çš„ä»»å‹™ï¼Œé€™é‚Šç›´æ¥æŠŠåˆ†é¡ç‚ºneutralè³‡æ–™æ‹¿æ‰ï¼Œå°ˆæ³¨åšæ­£è² å‘æƒ…æ„Ÿåˆ†æã€‚\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = []\n",
    "index_counter = 0\n",
    "\n",
    "for label in clean_data[\"Sentiment\"]:\n",
    "  if label != \"positive\" and label != \"negative\":\n",
    "    drop_list.append(index_counter)\n",
    "  index_counter += 1\n",
    "\n",
    "clean_data = clean_data.drop(index = drop_list, axis = 0)\n",
    "clean_data = clean_data.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨åˆ†é›¢è©•è«–è·Ÿç­”æ¡ˆ<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = list(clean_data[\"Summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label = clean_data[[\"Sentiment\"]].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 2. æº–å‚™ embeddings</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "&emsp;&ensp;ç‚ºäº†ç¯€çœå…§å­˜ï¼Œä¸‹é¢é¸æ“‡GloVeæä¾›æœ€å°çš„embeddingè³‡æ–™ä¾†ç”¨ã€‚\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "embeddings_index = dict()\n",
    "\n",
    "with open('glove.6B/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 3. padding</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "t_comment = Tokenizer()\n",
    "t_comment.fit_on_texts(comments)\n",
    "\n",
    "comment_vocab_size = len(t.word_index)\n",
    "hashed_comment = [one_hot(d, comment_vocab_size) for d in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10268\n",
      "[2852, 1954, 5053, 4089, 6852, 4867, 1112, 7124, 4421, 4458, 7859, 3779, 4867, 6264, 3803, 2690]\n"
     ]
    }
   ],
   "source": [
    "print(comment_vocab_size)\n",
    "print(hashed_comment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "max_length_eng = max([len(d.split()) for d in comments])\n",
    "print(max_length_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2852 1954 5053 4089 6852 4867 1112 7124 4421 4458 7859 3779 4867 6264\n",
      " 3803 2690    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "padded_comment = pad_sequences(hashed_comment, maxlen=max_length_eng, padding='post')\n",
    "print(padded_comment[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:14pt\">\n",
    "<b>Step 4. æ¨¡å‹è¨“ç·´</b>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "âœ¨å€åˆ†è¨“ç·´é›†è·Ÿæ¸¬è©¦é›†<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_eng_train, feature_eng_test, label_eng_train, label_eng_test = train_test_split(padded_comment, label, test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨åˆå§‹åŒ–æ¨¡å‹<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 108, 100)          1026800   \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 10800)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 10801     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,037,601\n",
      "Trainable params: 10,801\n",
      "Non-trainable params: 1,026,800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "eng_model = Sequential()\n",
    "e = Embedding(comment_vocab_size, 100, weights=[embedding_matrix], input_length=108, trainable=False)\n",
    "eng_model.add(e)\n",
    "eng_model.add(Flatten())\n",
    "eng_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "eng_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(eng_model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size:12pt\">\n",
    "âœ¨æ¨¡å‹è¨“ç·´åŠè©•ä¼°<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.900801\n",
      "Loss: 0\n"
     ]
    }
   ],
   "source": [
    "eng_model.fit(feature_eng_train, np.array(label_eng_train), epochs=50, verbose=0)\n",
    "\n",
    "eng_loss, eng_accuracy = eng_model.evaluate(feature_eng_test, np.array(label_eng_test), verbose=0)\n",
    "print('Accuracy: %f' % (eng_accuracy*100))\n",
    "print('Loss: %d' % eng_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\", style = \"font-size:16pt\">\n",
    "    ğŸ§<b>è¨è«–</b>ğŸ§<br>\n",
    "</p>\n",
    "<p style = \"font-size:12pt\">\n",
    "   &emsp;&emsp;å¾ä¸Šé¢æˆ‘å€‘å¯ä»¥çœ‹åˆ°æ¨¡å‹è¨“ç·´çš„çµæœé”åˆ°0.85ï¼Œå…¶å¯¦ç®—æ˜¯é‚„ä¸éŒ¯ã€‚ä½†å¦‚æœè·Ÿç•¶åˆä½¿ç”¨<code>nltk.vader</code>è¨ˆç®—çš„æƒ…æ„Ÿåˆ†æ•¸è¨“ç·´å‡ºä¾†çš„Naive Bayesæ¨¡å‹ï¼ˆaccuracy: 0.98ï¼‰ç›¸æ¯”ï¼Œå°±æœ‰é»å·®å¼·äººæ„äº†ã€‚æ¨æ¸¬æœƒé€™æ¨£ä¸»è¦çš„åŸå› æœ‰å…©å€‹ã€‚ç¬¬ä¸€å€‹æ˜¯<code>GloVe</code>æä¾›çš„embeddingæ˜¯ä»–å€‘é€éå„ç¨®è³‡æ–™é è¨“ç·´å®Œçš„ï¼Œä¸¦ä¸ç‰¹åˆ¥é‡å°ç‰¹å®šçš„åˆ†é¡ä»»å‹™è¨­è¨ˆã€‚å› æ­¤é›–ç„¶èƒ½å¤ æœ‰ä¸€å®šçš„æ°´æº–ï¼Œå»æ¯”è¼ƒé›£é”åˆ°åƒ<code>vadar</code>é€™æ¨£å°ˆé–€ç‚ºæƒ…æ„Ÿåˆ†æè¨­è¨ˆçš„å¥—ä»¶ç”¢å‡ºçš„ç‰¹å¾µæ‰€è¨“ç·´å‡ºçš„æ¨¡å‹é‚£éº¼å¥½çš„æˆæ•ˆã€‚ç¬¬äºŒå€‹åŸå› æ˜¯é€™æ¬¡æŒ‘é¸çš„embeddingè³‡æ–™ä¸å¤ å¤§ã€‚è¨“ç·´è³‡æ–™è¶Šå¤šèƒ½é”åˆ°è¶Šå¥½çš„æ•ˆæœæ˜¯æ©Ÿå™¨å­¸ç¿’çš„æ³•å‰‡ï¼Œæ‰€ä»¥ä¸æ’é™¤é‹ç”¨æ›´å¤§çš„embeddingè³‡æ–™å¯ä»¥é”åˆ°è·ŸNaive Bayesä¸¦é§•é½Šé©…çµæœçš„å¯èƒ½æ€§ã€‚\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
